"""
Strategy Signal Performance Test

Comprehensive performance testing and validation for the new strategy signal architecture.
"""

import asyncio
import time
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import logging
from datetime import datetime, timedelta

from trading.analysis.strategy_signal_engine import StrategySignalEngine
from trading.analysis.strategy_signal_backtester import StrategySignalBacktester
from trading.strategies.base.strategy_signal_factory import get_available_strategy_signals, create_strategy_signal


class StrategySignalPerformanceTester:
    """
    Comprehensive performance tester for strategy signal architecture.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.results = {}
    
    async def run_comprehensive_performance_test(self, 
                                               data_sizes: List[int] = [100, 500, 1000, 5000],
                                               iterations: int = 10) -> Dict[str, Any]:
        """
        Run comprehensive performance tests.
        
        Args:
            data_sizes: List of data sizes to test
            iterations: Number of iterations per test
            
        Returns:
            Performance test results
        """
        print("ğŸš€ Running Comprehensive Strategy Signal Performance Tests")
        print("=" * 70)
        
        all_results = {}
        
        for data_size in data_sizes:
            print(f"\nğŸ“Š Testing with {data_size} data points...")
            
            # Generate test data
            df = self._generate_test_data(data_size)
            
            # Test each strategy
            strategy_results = {}
            for strategy_type in get_available_strategy_signals():
                if strategy_type in ['delta_neutral', 'volatility']:  # Skip aliases
                    continue
                
                print(f"  Testing {strategy_type}...")
                result = await self._test_strategy_performance(df, strategy_type, iterations)
                strategy_results[strategy_type] = result
            
            all_results[f"{data_size}_points"] = strategy_results
        
        # Calculate summary statistics
        summary = self._calculate_performance_summary(all_results)
        
        return {
            'detailed_results': all_results,
            'summary': summary,
            'test_configuration': {
                'data_sizes': data_sizes,
                'iterations': iterations,
                'strategies_tested': list(strategy_results.keys())
            }
        }
    
    async def run_memory_usage_test(self, max_data_size: int = 10000) -> Dict[str, Any]:
        """
        Test memory usage patterns.
        
        Args:
            max_data_size: Maximum data size to test
            
        Returns:
            Memory usage test results
        """
        import psutil
        import os
        
        print("ğŸ§  Running Memory Usage Tests")
        print("=" * 40)
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        memory_results = {}
        
        for data_size in [1000, 5000, 10000]:
            if data_size > max_data_size:
                continue
            
            print(f"  Testing memory with {data_size} points...")
            
            # Generate data
            df = self._generate_test_data(data_size)
            
            # Create multiple strategy instances
            strategies = {}
            for strategy_type in ['reverse_delta_neutral', 'inventory_spot', 'volatility_harvesting']:
                strategies[strategy_type] = create_strategy_signal(strategy_type)
                await strategies[strategy_type].preload(df)
            
            current_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = current_memory - initial_memory
            
            memory_results[data_size] = {
                'total_memory_mb': current_memory,
                'additional_memory_mb': memory_usage,
                'memory_per_point_kb': (memory_usage * 1024) / data_size if data_size > 0 else 0
            }
            
            # Clean up
            del strategies
            del df
        
        return memory_results
    
    async def run_accuracy_validation_test(self) -> Dict[str, Any]:
        """
        Test accuracy and consistency of signal generation.
        
        Returns:
            Accuracy validation results
        """
        print("ğŸ¯ Running Accuracy Validation Tests")
        print("=" * 40)
        
        # Generate deterministic test data
        np.random.seed(42)
        df = self._generate_test_data(1000)
        
        accuracy_results = {}
        
        for strategy_type in ['reverse_delta_neutral', 'inventory_spot', 'volatility_harvesting']:
            print(f"  Validating {strategy_type}...")
            
            # Test consistency across multiple runs
            results = []
            for i in range(5):
                strategy = create_strategy_signal(strategy_type)
                await strategy.preload(df)
                df_with_signals = strategy.apply_signal_to_backtest(df)
                
                signal_counts = df_with_signals['signal'].value_counts().to_dict()
                results.append(signal_counts)
            
            # Check consistency
            consistent = all(r == results[0] for r in results)
            
            accuracy_results[strategy_type] = {
                'consistent_across_runs': consistent,
                'signal_distribution': results[0],
                'total_signals': sum(results[0].values())
            }
        
        return accuracy_results
    
    async def compare_with_legacy_performance(self) -> Dict[str, Any]:
        """
        Compare performance with legacy if/else approach (simulated).
        
        Returns:
            Performance comparison results
        """
        print("âš–ï¸ Comparing with Legacy Performance (Simulated)")
        print("=" * 50)
        
        df = self._generate_test_data(1000)
        iterations = 20
        
        # Test new architecture
        print("  Testing new strategy signal architecture...")
        new_times = []
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            engine = StrategySignalEngine()
            df_result = await engine.apply_signals_to_backtest(df, 'reverse_delta_neutral')
            
            end_time = time.perf_counter()
            new_times.append(end_time - start_time)
        
        # Simulate legacy approach timing
        print("  Simulating legacy if/else approach...")
        legacy_times = []
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            # Simulate legacy approach overhead
            strategy_type = 'reverse_delta_neutral'
            if strategy_type in ['reverse_delta_neutral', 'delta_neutral']:
                # Simulate processing overhead
                time.sleep(0.001)  # 1ms overhead for if/else chain
                result = df.copy()  # Simulate processing
            elif strategy_type == 'inventory_spot':
                time.sleep(0.001)
                result = df.copy()
            else:
                time.sleep(0.001)
                result = df.copy()
            
            end_time = time.perf_counter()
            legacy_times.append(end_time - start_time)
        
        new_avg = np.mean(new_times) * 1000  # Convert to ms
        legacy_avg = np.mean(legacy_times) * 1000
        
        improvement = ((legacy_avg - new_avg) / legacy_avg) * 100 if legacy_avg > 0 else 0
        
        return {
            'new_architecture_avg_ms': new_avg,
            'legacy_approach_avg_ms': legacy_avg,
            'performance_improvement_pct': improvement,
            'new_architecture_std_ms': np.std(new_times) * 1000,
            'legacy_approach_std_ms': np.std(legacy_times) * 1000,
            'iterations_tested': iterations
        }
    
    def _generate_test_data(self, size: int) -> pd.DataFrame:
        """
        Generate test market data.
        
        Args:
            size: Number of data points
            
        Returns:
            Test DataFrame
        """
        np.random.seed(42)  # For reproducible results
        
        dates = pd.date_range('2024-01-01', periods=size, freq='5T')
        base_price = 100.0
        
        # Generate realistic price movements
        price_changes = np.random.randn(size) * 0.1
        mexc_prices = base_price + np.cumsum(price_changes)
        
        df = pd.DataFrame({
            'MEXC_SPOT_bid_price': mexc_prices,
            'MEXC_SPOT_ask_price': mexc_prices + 0.05,
            'GATEIO_SPOT_bid_price': mexc_prices * 0.999,
            'GATEIO_SPOT_ask_price': mexc_prices * 0.999 + 0.05,
            'GATEIO_FUTURES_bid_price': mexc_prices * 0.998,
            'GATEIO_FUTURES_ask_price': mexc_prices * 0.998 + 0.05,
        }, index=dates)
        
        return df
    
    async def _test_strategy_performance(self, 
                                       df: pd.DataFrame, 
                                       strategy_type: str,
                                       iterations: int) -> Dict[str, Any]:
        """
        Test performance for a single strategy.
        
        Args:
            df: Test data
            strategy_type: Strategy to test
            iterations: Number of iterations
            
        Returns:
            Performance results for strategy
        """
        # Test signal generation performance
        signal_times = []
        backtest_times = []
        
        for i in range(iterations):
            # Test live signal generation
            market_data = {
                'mexc_bid': df['MEXC_SPOT_bid_price'].iloc[-1],
                'mexc_ask': df['MEXC_SPOT_ask_price'].iloc[-1],
                'gateio_spot_bid': df['GATEIO_SPOT_bid_price'].iloc[-1],
                'gateio_spot_ask': df['GATEIO_SPOT_ask_price'].iloc[-1],
                'gateio_futures_bid': df['GATEIO_FUTURES_bid_price'].iloc[-1],
                'gateio_futures_ask': df['GATEIO_FUTURES_ask_price'].iloc[-1]
            }
            
            strategy = create_strategy_signal(strategy_type)
            await strategy.preload(df)
            
            # Time live signal generation
            start_time = time.perf_counter()
            signal, confidence = strategy.generate_live_signal(market_data)
            end_time = time.perf_counter()
            signal_times.append(end_time - start_time)
            
            # Time backtest application
            start_time = time.perf_counter()
            df_result = strategy.apply_signal_to_backtest(df)
            end_time = time.perf_counter()
            backtest_times.append(end_time - start_time)
        
        return {
            'live_signal_avg_ms': np.mean(signal_times) * 1000,
            'live_signal_std_ms': np.std(signal_times) * 1000,
            'backtest_avg_ms': np.mean(backtest_times) * 1000,
            'backtest_std_ms': np.std(backtest_times) * 1000,
            'data_points_processed': len(df),
            'iterations': iterations
        }
    
    def _calculate_performance_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate summary statistics from performance results.
        
        Args:
            results: Detailed performance results
            
        Returns:
            Summary statistics
        """
        summary = {
            'fastest_strategy': {},
            'most_consistent_strategy': {},
            'scalability_analysis': {}
        }
        
        # Find fastest strategy for each data size
        for data_size, strategy_results in results.items():
            fastest_live = min(strategy_results.items(), 
                             key=lambda x: x[1]['live_signal_avg_ms'])
            fastest_backtest = min(strategy_results.items(), 
                                 key=lambda x: x[1]['backtest_avg_ms'])
            
            summary['fastest_strategy'][data_size] = {
                'live_signal': fastest_live[0],
                'live_signal_time_ms': fastest_live[1]['live_signal_avg_ms'],
                'backtest': fastest_backtest[0],
                'backtest_time_ms': fastest_backtest[1]['backtest_avg_ms']
            }
        
        return summary


async def run_full_performance_suite():
    """Run the complete performance test suite."""
    tester = StrategySignalPerformanceTester()
    
    print("ğŸ§ª Starting Comprehensive Strategy Signal Performance Test Suite")
    print("=" * 80)
    
    # Run all performance tests
    performance_results = await tester.run_comprehensive_performance_test()
    memory_results = await tester.run_memory_usage_test()
    accuracy_results = await tester.run_accuracy_validation_test()
    comparison_results = await tester.compare_with_legacy_performance()
    
    print("\nğŸ“Š PERFORMANCE TEST RESULTS SUMMARY")
    print("=" * 50)
    
    # Performance summary
    print("\nğŸš€ Performance Results:")
    summary = performance_results['summary']
    for data_size, fastest in summary['fastest_strategy'].items():
        print(f"  {data_size}: Fastest live signal - {fastest['live_signal']} ({fastest['live_signal_time_ms']:.2f}ms)")
    
    # Memory summary
    print("\nğŸ§  Memory Usage:")
    for size, memory in memory_results.items():
        print(f"  {size} points: {memory['additional_memory_mb']:.1f}MB ({memory['memory_per_point_kb']:.2f}KB/point)")
    
    # Accuracy summary
    print("\nğŸ¯ Accuracy Validation:")
    for strategy, accuracy in accuracy_results.items():
        consistency = "âœ… Consistent" if accuracy['consistent_across_runs'] else "âŒ Inconsistent"
        print(f"  {strategy}: {consistency} ({accuracy['total_signals']} total signals)")
    
    # Comparison summary
    print("\nâš–ï¸ Legacy Comparison:")
    print(f"  New architecture: {comparison_results['new_architecture_avg_ms']:.2f}ms")
    print(f"  Legacy approach: {comparison_results['legacy_approach_avg_ms']:.2f}ms")
    print(f"  Performance improvement: {comparison_results['performance_improvement_pct']:.1f}%")
    
    print("\nğŸ‰ Performance Test Suite Completed Successfully!")
    
    return {
        'performance': performance_results,
        'memory': memory_results,
        'accuracy': accuracy_results,
        'comparison': comparison_results
    }


if __name__ == "__main__":
    asyncio.run(run_full_performance_suite())