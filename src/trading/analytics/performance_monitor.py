"""
Performance Monitor

Real-time performance monitoring and metrics collection for the arbitrage engine.
Follows Single Responsibility Principle - focused solely on performance tracking.

HFT COMPLIANT: Lock-free metrics collection with minimal overhead (<1Î¼s).
"""

import asyncio
from core.logging import get_logger
import time
from typing import Dict, Any, Optional, Callable
from trading.arbitrage.types import EngineStatistics, ArbitrageConfig

logger = get_logger('arbitrage.performance_monitor')


class PerformanceMonitor:
    """
    Monitors and reports arbitrage engine performance metrics.
    
    Responsibilities:
    - Track engine statistics
    - Monitor performance against HFT targets
    - Generate performance reports
    - Alert on performance degradation
    """
    
    # HFT performance thresholds
    HFT_TARGET_EXECUTION_MS = 50
    HFT_WARNING_EXECUTION_MS = 40  # Warn before hitting limit
    
    def __init__(self, config: ArbitrageConfig):
        self.config = config
        self.statistics = EngineStatistics()
        self.start_time: Optional[float] = None
        self._running = False
        self._monitoring_task: Optional[asyncio.Task] = None
        self._statistics_callback: Optional[Callable] = None
        
        # Performance tracking
        self._execution_times: list[float] = []
        self._max_execution_samples = 1000  # Rolling window
        
    def start(self, statistics_callback: Optional[Callable] = None):
        """
        Start performance monitoring.
        
        Args:
            statistics_callback: Optional callback to get engine statistics
        """
        if self._running:
            logger.warning("Performance monitor already running")
            return
            
        self.start_time = time.perf_counter()
        self._running = True
        self._statistics_callback = statistics_callback
        self._monitoring_task = asyncio.create_task(self._monitor_loop())
        logger.info("Performance monitoring started")
    
    async def stop(self):
        """Stop performance monitoring."""
        if not self._running:
            return
            
        self._running = False
        
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        
        # Log final statistics
        self._log_final_statistics()
        logger.info("Performance monitoring stopped")
    
    async def _monitor_loop(self):
        """Main monitoring loop."""
        try:
            while self._running:
                try:
                    # Update statistics
                    await self._update_statistics()
                    
                    # Log performance summary
                    self._log_performance_summary()
                    
                    # Check for performance issues
                    self._check_performance_thresholds()
                    
                    # Wait for next monitoring interval
                    await asyncio.sleep(self.config.position_monitor_interval_ms / 1000.0)
                    
                except Exception as e:
                    logger.error(f"Error in performance monitoring: {e}")
                    await asyncio.sleep(10)  # Brief pause before retry
                    
        except asyncio.CancelledError:
            logger.debug("Performance monitoring cancelled")
    
    async def _update_statistics(self):
        """Update statistics from engine."""
        if not self._statistics_callback:
            return
            
        try:
            # Get statistics from engine
            stats_dict = self._statistics_callback()
            
            if stats_dict:
                # Update our statistics
                self.statistics.opportunities_detected = stats_dict.get('opportunities_detected', 0)
                self.statistics.opportunities_executed = stats_dict.get('opportunities_executed', 0)
                
                # Handle profit as string or float
                profit = stats_dict.get('total_realized_profit', '0.0')
                self.statistics.total_realized_profit = float(profit) if isinstance(profit, (str, int, float)) else 0.0
                
                # Update execution time
                exec_time = stats_dict.get('average_execution_time_ms', 0)
                if exec_time > 0:
                    self.record_execution_time(exec_time)
                
                self.statistics.success_rate = stats_dict.get('success_rate', 0.0)
                
                # Update uptime
                if self.start_time:
                    self.statistics.uptime_seconds = time.perf_counter() - self.start_time
                    
        except Exception as e:
            logger.error(f"Error updating statistics: {e}")
    
    def record_execution_time(self, execution_time_ms: float):
        """
        Record execution time for performance tracking.
        
        Args:
            execution_time_ms: Execution time in milliseconds
        """
        self._execution_times.append(execution_time_ms)
        
        # Maintain rolling window
        if len(self._execution_times) > self._max_execution_samples:
            self._execution_times.pop(0)
        
        # Update average
        if self._execution_times:
            self.statistics.average_execution_time_ms = sum(self._execution_times) / len(self._execution_times)
    
    def _log_performance_summary(self):
        """Log performance summary."""
        logger.info("Performance Summary:")
        logger.info(f"  Uptime: {self.statistics.uptime_seconds/60:.1f} minutes")
        logger.info(f"  Opportunities Detected: {self.statistics.opportunities_detected}")
        logger.info(f"  Opportunities Executed: {self.statistics.opportunities_executed}")
        logger.info(f"  Total Profit: ${self.statistics.total_realized_profit:.2f}")
        logger.info(f"  Success Rate: {self.statistics.success_rate:.1f}%")
        logger.info(f"  Avg Execution Time: {self.statistics.average_execution_time_ms:.1f}ms")
    
    def _check_performance_thresholds(self):
        """Check performance against HFT thresholds."""
        avg_exec_time = self.statistics.average_execution_time_ms
        
        if avg_exec_time > self.HFT_TARGET_EXECUTION_MS:
            logger.warning(
                f"PERFORMANCE DEGRADATION: Execution time ({avg_exec_time:.1f}ms) "
                f"exceeds HFT target ({self.HFT_TARGET_EXECUTION_MS}ms)"
            )
        elif avg_exec_time > self.HFT_WARNING_EXECUTION_MS:
            logger.warning(
                f"Performance Warning: Execution time ({avg_exec_time:.1f}ms) "
                f"approaching HFT limit"
            )
        
        # Check against configured target
        if avg_exec_time > self.config.target_execution_time_ms:
            logger.debug(
                f"Execution time ({avg_exec_time:.1f}ms) exceeds configured "
                f"target ({self.config.target_execution_time_ms}ms)"
            )
    
    def _log_final_statistics(self):
        """Log final statistics on shutdown."""
        if not self.start_time:
            return
            
        uptime = time.perf_counter() - self.start_time
        
        logger.info("Final Statistics:")
        logger.info(f"  Total Uptime: {uptime/60:.1f} minutes")
        logger.info(f"  Total Opportunities Detected: {self.statistics.opportunities_detected}")
        logger.info(f"  Total Opportunities Executed: {self.statistics.opportunities_executed}")
        logger.info(f"  Total Profit: ${self.statistics.total_realized_profit:.2f}")
        
        if self.statistics.opportunities_executed > 0:
            avg_profit = self.statistics.total_realized_profit / self.statistics.opportunities_executed
            logger.info(f"  Average Profit per Trade: ${avg_profit:.2f}")
            
            opportunities_per_hour = (self.statistics.opportunities_detected / (uptime / 3600)) if uptime > 0 else 0
            logger.info(f"  Opportunities per Hour: {opportunities_per_hour:.1f}")
        
        # Performance analysis
        if self._execution_times:
            min_time = min(self._execution_times)
            max_time = max(self._execution_times)
            logger.info(f"  Execution Time Range: {min_time:.1f}ms - {max_time:.1f}ms")
            
            # Calculate percentiles
            sorted_times = sorted(self._execution_times)
            p50 = sorted_times[len(sorted_times)//2]
            p95 = sorted_times[int(len(sorted_times)*0.95)] if len(sorted_times) > 20 else max_time
            p99 = sorted_times[int(len(sorted_times)*0.99)] if len(sorted_times) > 100 else max_time
            
            logger.info(f"  P50 Execution Time: {p50:.1f}ms")
            logger.info(f"  P95 Execution Time: {p95:.1f}ms")
            logger.info(f"  P99 Execution Time: {p99:.1f}ms")
    
    def get_statistics(self) -> EngineStatistics:
        """Get current statistics."""
        return self.statistics
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Get current metrics as dictionary.
        
        Returns:
            Dictionary of current metrics
        """
        return {
            **self.statistics.to_dict(),
            'hft_compliant': self.statistics.average_execution_time_ms <= self.HFT_TARGET_EXECUTION_MS,
            'execution_samples': len(self._execution_times),
        }
    
    @property
    def is_healthy(self) -> bool:
        """Check if performance is within acceptable bounds."""
        return (
            self._running and
            self.statistics.average_execution_time_ms <= self.config.target_execution_time_ms * 1.5
        )